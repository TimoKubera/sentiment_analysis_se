{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<center>\n","    <img src=\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/assets/logos/SN_web_lightmode.png\" width=\"300\" alt=\"cognitiveclass.ai logo\">\n","</center>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# **Types of Sequential Data**\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Estimated time needed: **45** minutes\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In common machine learning tasks, it is assumed that the data is identically and independently distributed (i.i.d.). However, when dealing with changes in distribution of the underlying data generating process, or working with data that has temporal dependence, this i.i.d. assumption breaks.\n","\n","Practitioners and data scientists should be able to model such data by drawing from a variety of tools for sequential data analysis. In this lab, we introduce forms of sequential data, and basic concepts needed to understand components of a time-series.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## **Table of Contents**\n","\n","<ol>\n","    <li><a href=\"https://#Objectives\">Objectives</a></li>\n","    <li><a href=\"https://#Datasets\">Datasets</a></li>\n","    <li>\n","        <a href=\"https://#Setup\">Setup</a>\n","        <ol>\n","            <li><a href=\"https://#Installing-Required-Libraries\">Installing Required Libraries</a></li>\n","            <li><a href=\"https://#Importing-Required-Libraries\">Importing Required Libraries</a></li>\n","            <li><a href=\"https://#Defining-Helper-Functions\">Defining Helper Functions</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"https://#Background\">Background</a>\n","        <ol></ol>\n","    </li>\n","    <li><a href=\"https://#The i.i.d. Assumption\">The i.i.d. Assumption</a>\n","        <ol></ol>\n","    </li>\n","    <li><a href=\"https://#Understanding Sequential Modeling\">Understanding Sequential Modeling</a>\n","        <ol>\n","        </ol>\n","    </li>\n","    <li><a href=\"https://#Forms of Sequential Data\">Forms of Sequential Data</a>\n","        <ol>\n","            <li><a href=\"https://#Working with time-series data\">Working with time-series data</a></li>\n","            <li><a href=\"https://#Working with text data\">Working with text data</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"https://#Example 1 - Working with Speech Commands\">Example 1 - Working with Speech Commands</a>\n","        <ol>\n","            <li><a href=\"https://#Exercise A - Import data set\">Exercise A - Import data set</a></li>\n","            <li><a href=\"https://#Exercise B - Extract audio clips\">Exercise B - Extract audio clips</a></li>\n","            <li><a href=\"https://#Exercise C - Plotting audio clips\">Exercise C - Plotting audio clips</a></li>\n","            <li><a href=\"https://#Exercise D - Basic pre-processing\">Exercise D - Basic pre-processing</a></li>\n","            <li><a href=\"https://#Exercise E - Spectrogram\">Exercise E - Spectrogram</a></li>\n","        </ol>\n","    </li>\n","    <li><a href=\"https://#Example 2 - Gene Family Classification\">Example 2 - Gene Family Classification</a>\n","        <ol>\n","            <li><a href=\"https://#Exercise A - Load human DNA data\">Exercise A - Load human DNA data</a></li>\n","            <li><a href=\"https://#Exercise B - Sequences of Characters into K-mers\">Exercise B - Sequences of Characters into K-mers</a></li>\n","            <li><a href=\"https://#Exercise C - Bag-of-Words\">Exercise C - Bag-of-Words</a></li>\n","            <li><a href=\"https://#Exercise D - Multinomial Naïve Bayes Classifier\">Exercise D - Multinomial Naïve Bayes Classifier</a></li>\n","        </ol>\n","    </li>\n","</ol>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Objectives\n","\n","After completing this lab you will be able to:\n","\n","*   Describe various forms of sequential data, and common tasks that can be modeled using sequential data\n","*   Decompose a time-series and perform time-series imputation\n","*   Pre-process and vectorize a text dataset\n","*   Pre-process and visualize an audio dataset, and create spectrograms\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["***\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Setup\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["For this lab, we will be using the following libraries:\n","\n","*   [`pandas`](https://pandas.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for managing the data.\n","*   [`numpy`](https://numpy.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for mathematical operations.\n","*   [`sklearn`](https://scikit-learn.org/stable/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for machine learning and machine-learning-pipeline related functions.\n","*   [`seaborn`](https://seaborn.pydata.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for visualizing the data.\n","*   [`matplotlib`](https://matplotlib.org/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMML0187ENSkillsNetwork31430127-2021-01-01) for additional plotting tools.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Installing Required Libraries\n","\n","The following required libraries are pre-installed in the Skills Network Labs environment. However, if you run these notebook commands in a different Jupyter environment (like Watson Studio or Ananconda), you will need to install these libraries by removing the `#` sign before `!mamba` in the following code cell.\n"]},{"cell_type":"code","execution_count":95,"metadata":{},"outputs":[],"source":["# All Libraries required for this lab are listed below. The libraries pre-installed on Skills Network Labs are commented.\n","# !mamba install -qy pandas==1.3.4 numpy==1.21.4 seaborn==0.9.0 matplotlib==3.5.0 scikit-learn==0.20.1\n","# Note: If your environment doesn't support \"!mamba install\", use \"!pip install\""]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The following required libraries are **not** pre-installed in the Skills Network Labs environment. **You will need to run the following cell** to install them:\n"]},{"cell_type":"code","execution_count":96,"metadata":{},"outputs":[],"source":["%%capture\n","\n","!pip install nltk "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Importing Required Libraries\n","\n","*We recommend you import all required libraries in one place (here):*\n"]},{"cell_type":"code","execution_count":97,"metadata":{},"outputs":[{"name":"stdout","output_type":"stream","text":["2.12.0\n"]},{"name":"stderr","output_type":"stream","text":["[nltk_data] Downloading package punkt to /Users/timo/nltk_data...\n","[nltk_data]   Package punkt is already up-to-date!\n","[nltk_data] Downloading package wordnet to /Users/timo/nltk_data...\n","[nltk_data]   Package wordnet is already up-to-date!\n","[nltk_data] Downloading package stopwords to /Users/timo/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n","[nltk_data] Downloading package omw-1.4 to /Users/timo/nltk_data...\n","[nltk_data]   Package omw-1.4 is already up-to-date!\n"]}],"source":["import warnings\n","warnings.simplefilter('ignore')\n","\n","import os\n","os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2' \n","\n","import numpy as np\n","import pandas as pd\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","%matplotlib inline\n","import tensorflow as tf\n","print(tf. __version__)\n","import skillsnetwork\n","\n","import keras \n","from sklearn.model_selection import train_test_split\n","from sklearn.tree import DecisionTreeRegressor\n","from sklearn.model_selection import ShuffleSplit\n","from sklearn.model_selection import cross_val_score\n","from statsmodels.tsa.seasonal import seasonal_decompose\n","from dateutil.parser import parse\n","import os\n","import pathlib\n","from scipy import signal\n","from scipy.io import wavfile\n","import re\n","import string\n","import nltk\n","nltk.download('punkt')\n","nltk.download('wordnet')\n","nltk.download('stopwords')\n","nltk.download('omw-1.4')\n","from nltk.corpus import stopwords\n","from nltk.tokenize import word_tokenize, sent_tokenize\n","from nltk.stem import WordNetLemmatizer\n","from sklearn.feature_extraction.text import CountVectorizer\n","from sklearn.feature_extraction.text import TfidfVectorizer\n","from sklearn.feature_extraction.text import HashingVectorizer\n","\n","sns.set_context('notebook')\n","sns.set_style('white')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Background\n","\n","In machine learning, it is quite common to assume that the data are identically and independently distributed or i.i.d. This implies that the generative process does not have any memory of past samples to generate new samples.\n","This assumption is usually violated when dealing with sequential data as samples depend on past information, that is, they are reliant on one another due to their sequential order.\n","Examples where data depends on past information may include weather. For instance, it's not likely to snow if the temperature is extremely hot the day before. In financial time series, the price of the stock is not only determined by the fundamentals of the company but the price of the stock the day before; we will see more examples later on.\n","There are many method to study time series data; in this section, we will study some of the classic methods. These methods are precursors to Recurrent Neural Networks (RNNs), a well-known method, well-suited to model these forms of sequential data that we will cover in the next few sections.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## The i.i.d. Assumption\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this lab, we will be dealing with data that, rather than being drawn i.i.d., from some joint distribution $P(x, y)$, actually consists of sequences of $(x, y)$ pairs that show some sequential correlation. This means that values close to the x and y values are likely to be related to and/or dependent on each other.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["So when the points in the data set are dependent on the other points in the data set, the data is termed sequential.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The sequential supervised learning problem can be formulated as follows:\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let ${(x_i, y_i)}^N\\_{i=1}$ be a set of N training examples. In a part-of-speech tagging task, one $(x_i, y_i)$ pair might consist of $x_i =$ do you want fries and $y_i =$ verb pronoun verb noun. Our goal is to build a model, $h$, that predicts the next label sequence, $y = h(x)$, given an input sequence, $x$. Here the task is to predict the $t+1$st element of the sequence $(y\\_1,...,y_t)$.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us walk through an example to highlight the issues when working with non-i.i.d. data. We will be using financial quotations from various energy companies.\n"]},{"cell_type":"code","execution_count":98,"metadata":{},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"f94bdbe64b314d3495c03d674dedc175","version_major":2,"version_minor":0},"text/plain":["Downloading financial-data.zip:   0%|          | 0/46394 [00:00<?, ?it/s]"]},"metadata":{},"output_type":"display_data"},{"ename":"FileExistsError","evalue":"Error: File '/Users/timo/Downloads/financial-data' already exists.\nIf you want to overwrite any existing files, use prepare(..., overwrite=True).","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileExistsError\u001b[0m                           Traceback (most recent call last)","Cell \u001b[0;32mIn[98], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mawait\u001b[39;00m skillsnetwork\u001b[39m.\u001b[39mprepare(\u001b[39m\"\u001b[39m\u001b[39mhttps://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/financial-data.zip\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/skillsnetwork/core.py:292\u001b[0m, in \u001b[0;36mprepare\u001b[0;34m(url, path, verbose, overwrite)\u001b[0m\n\u001b[1;32m    290\u001b[0m         shutil\u001b[39m.\u001b[39mmove(tmp_download_file, extract_dir \u001b[39m/\u001b[39m filename)\n\u001b[1;32m    291\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m \u001b[39mas\u001b[39;00m e:\n\u001b[0;32m--> 292\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mFileExistsError\u001b[39;00m(\n\u001b[1;32m    293\u001b[0m         \u001b[39mstr\u001b[39m(e)\n\u001b[1;32m    294\u001b[0m         \u001b[39m+\u001b[39m \u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mIf you want to overwrite any existing files, use prepare(..., overwrite=True).\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m    295\u001b[0m     ) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m# If in jupyterlite environment, the extract_dir = path, so the files are already there.\u001b[39;00m\n\u001b[1;32m    298\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m _is_jupyterlite():\n\u001b[1;32m    299\u001b[0m     \u001b[39m# If not in jupyterlite environment, symlink top-level file objects in extract_dir\u001b[39;00m\n","\u001b[0;31mFileExistsError\u001b[0m: Error: File '/Users/timo/Downloads/financial-data' already exists.\nIf you want to overwrite any existing files, use prepare(..., overwrite=True)."]}],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/financial-data.zip\")\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import pandas as pd\n","\n","symbols = {\"TOT\": \"Total\", \"XOM\": \"Exxon\", \"CVX\": \"Chevron\",\n","           \"COP\": \"ConocoPhillips\", \"VLO\": \"Valero Energy\"}\n","template_name = (\"./financial-data/{}.csv\")\n","\n","quotes = {}\n","for symbol in symbols:\n","    data = pd.read_csv(\n","        template_name.format(symbol), index_col=0, parse_dates=True\n","    )\n","    quotes[symbols[symbol]] = data[\"open\"]\n","quotes = pd.DataFrame(quotes)\n","quotes .head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us plot a few financial quotations.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["import matplotlib.pyplot as plt\n","\n","quotes.plot()\n","plt.ylabel(\"Quote value\")\n","plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n","_ = plt.title(\"Stock values over time\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We want to predict the quotation of Chevron using all other energy companies’ quotes. We will use the decision tree regressor that is expected to overfit and thus not generalize to unseen data. We will start off by splitting our data into a testing and training set.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data, target = quotes.drop(columns=[\"Chevron\"]), quotes[\"Chevron\"]\n","data_train, data_test, target_train, target_test = train_test_split(\n","    data, target, shuffle=True, random_state=0)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["we have our features\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["and target Chevron\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let’s now define our model:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regressor = DecisionTreeRegressor()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["And now we apply a cross-validation strategy, to check the generalization performance of our model.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv = ShuffleSplit(random_state=0)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["regressor.fit(data_train, target_train)\n","target_predicted = regressor.predict(data_test)\n","# Affect the index of `target_predicted` to ease the plotting\n","target_predicted = pd.Series(target_predicted, index=target_test.index)\n","target_predicted "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we perform evaluation.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import r2_score\n","\n","test_score = r2_score(target_test, target_predicted)\n","print(f\"The R2 on this single split is: {test_score:.2f}\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We get outstanding generalization performance in terms of $R^2$. Let's plot the predictions against the ground truth.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_train.plot(label=\"Training\")\n","target_test.plot(label=\"Testing\")\n","target_predicted.plot(label=\"Prediction\")\n","\n","plt.ylabel(\"Quote value\")\n","plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n","_ = plt.title(\"Model predictions using a ShuffleSplit strategy\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The results are surprisingly very good, as we had originally expected the model to overfit and not generalize on unseen test sets. Let us now investigate why this is the case.\n","\n","We used a cross-validation method that shuffles data and splits. We will simplify this procedure by not shuffling our data and plotting the results obtained.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["data_train, data_test, target_train, target_test = train_test_split(\n","    data, target, shuffle=False, random_state=0,\n",")\n","regressor.fit(data_train, target_train)\n","target_predicted = regressor.predict(data_test)\n","target_predicted = pd.Series(target_predicted, index=target_test.index)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_score = r2_score(target_test, target_predicted)\n","print(f\"The R2 on this single split is: {test_score:.2f}\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["target_train.plot(label=\"Training\")\n","target_test.plot(label=\"Testing\")\n","target_predicted.plot(label=\"Prediction\")\n","\n","plt.ylabel(\"Quote value\")\n","plt.legend(bbox_to_anchor=(1.05, 0.8), loc=\"upper left\")\n","_ = plt.title(\"Model predictions using a split without shuffling\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we see that our model performs worse than just predicting the mean of the target. Why is that?\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this dataset, and all other similar time series datasets, the testing samples follow some of the training sample. As mentioned before, in time-series subsequent samples are dependent on previous samples; that is, there is a relationship between sample $t$ and sample $t+1$. We are clearly violating the i.i.d. assumption. The model's prediction would be close to true values even when it doesn't learn anything, because of data leakage and because it could have just memorized the training dataset.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Understanding Sequential Modeling\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Sequential modeling can be described as the process of forecasting a sequence of values from a set of input values, similar to what we saw in the previous section.\n","\n","For example, consider a time-series that represents how a patient's physiological values like continuous heart rate, ECG, breathing rate, and temperature, change over time. One possible sequential modeling task could be to forecast their disease trajectory or predict their medical condition at some future time.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Text prediction is another example. For example, you can be given a sequence of words, and your modeling task could be to predict the next word based on the sequence of the previous word/phrase and a set of pre-loaded conditions and rules.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Forms of Sequential Data\n","\n","Sequential data contains elements that are ordered into sequences. For example, time series (like stock values or sensor measurements), gene sequences (${C,G,A,T}$), speech, text (${a,...,z,0,...,9,...}$), video clips, and musical notes, and so on.\n","\n","To summarize, sequential data has some temporal coherence, and can be of arbitrary lengths. A lot of tasks can be modeled from these types of data. For example:\n","\n","*   text classification, such as spam email or not\n","*   language translation, such as French to English\n","*   time-series forecasting, such as stock prices prediction\n","\n","Let us look at a few common sequential data sets, and understand pre-processing techniques associated with each.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Working with time-series data\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Time series are special types of sequences that consist of random variables indexed by time. In particular, the random variables can be dependent and their distribution might change over time, so time-series also violate the i.i.d. assumption.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Time-series decomposition\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Time series data can exhibit various patterns, and it is often helpful to split a time series into several components, each representing an underlying pattern category. These include trend, seasonality, and cycles, which we will now explain using examples.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*   **trend**: A trend is observed when there is an increasing or decreasing slope observed in the time series. In the following time series, we see an increasing pattern.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["trend = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/guinearice.csv', parse_dates=['date'], index_col='date')\n","plt.plot(trend.values)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*   **seasonality**: Seasonality represents a distinct repeated pattern observed between regular intervals due to seasonal factors, like month of the year, the day of the month, weekdays or even time of the day, festivals, and so on.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seasonality = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/sunspotarea.csv', parse_dates=['date'], index_col='date')\n","plt.plot(seasonality.value)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A time series can have both trend and seasonality, for example:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["seasonality_trend = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/AirPassengers.csv', parse_dates=['date'], index_col='date')\n","plt.plot(seasonality_trend.value)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*   **cyclic**: Cyclicity is similar to seasonality, but it happens when the rise and fall pattern in the series does not happen in fixed calendar-based intervals, that is, if the patterns are not of fixed calendar-based frequencies, then the pattern is cyclic.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A time series can be modeled as an additive or multiplicative, wherein each observation in the series can be expressed as either a sum or a product of the components.\n","\n","**Additive time series** \\\n","\\\n","$Value(t) = BaseLevel(t) + Trend(t) + Seasonality(t) + Error(t)$\n","\n","**Multiplicative Time Series** \\\n","\\\n","$Value(t) = BaseLevel (t)x Trend(t) x Seasonality(t) x Error(t)$\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can use functions like `seasonal_decompose` from the python package, `statsmodel`, to decompose our time-series into trend, seasonality, and the residual components.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Import Data\n","df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/a10.csv', parse_dates=['date'], index_col='date')\n","plt.plot(df.value)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us start by performing additive decomposition. Note that by using `extrapolate_trend = 'freq'`, we impute missing values at the beginning of the time series.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Additive Decomposition\n","result_add = seasonal_decompose(df['value'], model='additive', extrapolate_trend='freq')\n","\n","# Plot\n","result_add.plot()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, similarly we can try performing multiplicative decomposition:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Multiplicative Decomposition \n","result_mul = seasonal_decompose(df['value'], model='multiplicative', extrapolate_trend='freq')\n","\n","#Plot\n","result_mul.plot()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this particular case, we see that with additive decomposition, some pattern is still left over, while with multiplicative decomposition, the result looks quite random, which is what we want.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["So how do we detrend our time series? One way would be to subtract the trend component obtained from the time series decomposition we saw earlier.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["detrended = df.value.values - result_mul.trend\n","plt.plot(detrended)\n","plt.title('Drug Sales detrended by subtracting the trend component', fontsize=16)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Similarly, we can remove the seasonality component:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["deseasonalized = df.value.values / result_mul.seasonal\n","\n","# Plot\n","plt.plot(deseasonalized)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Time-series imputation\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Earlier, we briefly mentioned imputing missing values in a time-series. Sometimes, your time series will have missing dates/times. That means the data was not captured or unavailable for those periods.\n","\n","Depending on the nature of the series, we can try multiple approaches for imputation:\n","\n","*   Forward\n","*   Backward Fill\n","*   Linear Interpolation, etc.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us start by parsing the data frame indices as dates.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/a10.csv', parse_dates=['date'], index_col='date')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*   **Forward Imputation**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.interpolate import interp1d\n","from sklearn.metrics import mean_squared_error\n","\n","df_ffill = df.ffill()\n","# Print the MSE between imputed value and ground truth\n","error = np.round(mean_squared_error(df['value'], df_ffill['value']), 2)\n","df_ffill['value'].plot(title='Forward Fill (MSE: ' + str(error) +\")\", label='Forward Fill', style=\".-\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*   **Backward Imputation**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df_bfill = df.bfill()\n","error = np.round(mean_squared_error(df['value'], df_bfill['value']), 2)\n","df_bfill['value'].plot(title=\"Backward Fill (MSE: \" + str(error) +\")\", label='Back Fill', color='firebrick', style=\".-\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["*   **Linear Interpolation**\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['rownum'] = np.arange(df.shape[0])\n","df_nona = df.dropna(subset = ['value'])\n","f = interp1d(df_nona['rownum'], df_nona['value'])\n","df['linear_fill'] = f(df['rownum'])\n","error = np.round(mean_squared_error(df['value'], df['linear_fill']), 2)\n","df['linear_fill'].plot(title=\"Linear Fill (MSE: \" + str(error) +\")\",label='Cubic Fill', color='green', style=\".-\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Time-series decomposition and imputation are common pre-processing steps used when working on time-series prediction or forecasting tasks.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Working with text data\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this section, we will be working with a portion of the [Twitter Sentiment Analysis](https://www.kaggle.com/datasets/jp797498e/twitter-entity-sentiment-analysis?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) dataset from Kaggle.\n","\n","> Given a message and an entity, the task is to judge the sentiment of the message about the entity. There are three classes in this dataset: Positive, Negative, and Neutral.\n","\n","The file contains the tweet along with the labels. For the purpose of this lab, we will just explore the dataset, and perform some basic pre-processing.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["First, let us download this dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cols = ['Id', 'Entity', 'Sentiment', 'Tweet']\n","df = pd.read_csv('https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/twitter_validation.csv', names = cols, header=None)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.head(2)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Visualizing\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using `groupby` in pandas, we can determine the number of tweets present in each category and plot them.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["fig = plt.figure(figsize=(8,6))\n","\n","df.groupby(['Sentiment']).Tweet.count().sort_values().plot.barh(\n","    ylim=0, title= 'tweets per category')\n","\n","plt.xlabel('# of occurrences', fontsize = 12);"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Cleaning up\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we will focus on preprocessing the raw tweets. This step is important because raw tweets without preprocessing are highly unstructured and contain redundant and often problematic information. They contain tons of noise that should be removed.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Most of the text data is cleaned by following the steps below.\n","\n","*   Remove punctuations\n","*   Remove stopwords\n","*   Tokenization - Converting a sentence into list of words\n","*   Lemmatization and stemming - Tranforming any form of a word to its root word\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Using a corpus of stop-words, including articles, prepositions, and conjunctions, from the Natural Language Toolkit (NLTK) suite, words with little lexical content (such as the, a, also, from, and so on) can be filtered out, for more check out this <a href=\"https://realpython.com/nltk-nlp-python/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01\">link</a>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[{"ename":"AttributeError","evalue":"'list' object has no attribute 'strip'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","Cell \u001b[0;32mIn[94], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m             text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(punc, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[1;32m      6\u001b[0m     \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39mstrip()\u001b[39m.\u001b[39mlower()\n\u001b[0;32m----> 8\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTweet\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m df[\u001b[39m'\u001b[39;49m\u001b[39mTweet\u001b[39;49m\u001b[39m'\u001b[39;49m]\u001b[39m.\u001b[39;49mapply(remove_punctuation)\n\u001b[1;32m      9\u001b[0m df[\u001b[39m'\u001b[39m\u001b[39mTweet\u001b[39m\u001b[39m'\u001b[39m]\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/series.py:4771\u001b[0m, in \u001b[0;36mSeries.apply\u001b[0;34m(self, func, convert_dtype, args, **kwargs)\u001b[0m\n\u001b[1;32m   4661\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mapply\u001b[39m(\n\u001b[1;32m   4662\u001b[0m     \u001b[39mself\u001b[39m,\n\u001b[1;32m   4663\u001b[0m     func: AggFuncType,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4666\u001b[0m     \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs,\n\u001b[1;32m   4667\u001b[0m ) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m DataFrame \u001b[39m|\u001b[39m Series:\n\u001b[1;32m   4668\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   4669\u001b[0m \u001b[39m    Invoke function on values of Series.\u001b[39;00m\n\u001b[1;32m   4670\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   4769\u001b[0m \u001b[39m    dtype: float64\u001b[39;00m\n\u001b[1;32m   4770\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[0;32m-> 4771\u001b[0m     \u001b[39mreturn\u001b[39;00m SeriesApply(\u001b[39mself\u001b[39;49m, func, convert_dtype, args, kwargs)\u001b[39m.\u001b[39;49mapply()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1123\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1120\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mapply_str()\n\u001b[1;32m   1122\u001b[0m \u001b[39m# self.f is Callable\u001b[39;00m\n\u001b[0;32m-> 1123\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mapply_standard()\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/core/apply.py:1174\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1172\u001b[0m     \u001b[39melse\u001b[39;00m:\n\u001b[1;32m   1173\u001b[0m         values \u001b[39m=\u001b[39m obj\u001b[39m.\u001b[39mastype(\u001b[39mobject\u001b[39m)\u001b[39m.\u001b[39m_values\n\u001b[0;32m-> 1174\u001b[0m         mapped \u001b[39m=\u001b[39m lib\u001b[39m.\u001b[39;49mmap_infer(\n\u001b[1;32m   1175\u001b[0m             values,\n\u001b[1;32m   1176\u001b[0m             f,\n\u001b[1;32m   1177\u001b[0m             convert\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mconvert_dtype,\n\u001b[1;32m   1178\u001b[0m         )\n\u001b[1;32m   1180\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(mapped) \u001b[39mand\u001b[39;00m \u001b[39misinstance\u001b[39m(mapped[\u001b[39m0\u001b[39m], ABCSeries):\n\u001b[1;32m   1181\u001b[0m     \u001b[39m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[1;32m   1182\u001b[0m     \u001b[39m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[1;32m   1183\u001b[0m     \u001b[39mreturn\u001b[39;00m obj\u001b[39m.\u001b[39m_constructor_expanddim(\u001b[39mlist\u001b[39m(mapped), index\u001b[39m=\u001b[39mobj\u001b[39m.\u001b[39mindex)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/pandas/_libs/lib.pyx:2924\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[0;34m()\u001b[0m\n","Cell \u001b[0;32mIn[94], line 6\u001b[0m, in \u001b[0;36mremove_punctuation\u001b[0;34m(text)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[39mif\u001b[39;00m punc \u001b[39min\u001b[39;00m text:\n\u001b[1;32m      5\u001b[0m         text \u001b[39m=\u001b[39m text\u001b[39m.\u001b[39mreplace(punc, \u001b[39m'\u001b[39m\u001b[39m \u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m----> 6\u001b[0m \u001b[39mreturn\u001b[39;00m text\u001b[39m.\u001b[39;49mstrip()\u001b[39m.\u001b[39mlower()\n","\u001b[0;31mAttributeError\u001b[0m: 'list' object has no attribute 'strip'"]}],"source":["def remove_punctuation(text):\n","    regular_punct = list(string.punctuation)\n","    for punc in regular_punct:\n","        if punc in text:\n","            text = text.replace(punc, ' ')\n","    return text.strip().lower()\n","\n","df['Tweet'] = df['Tweet'].apply(remove_punctuation)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def remove_stopwords(tweet):\n","    en_stops = set(stopwords.words('english'))\n","    tweet = tweet.split()\n","    tweet = \" \".join([word for word in tweet if not word in en_stops])  \n","    return tweet\n","\n","df['Tweet'] = df['Tweet'].apply(remove_stopwords)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Tweets can be further processed with tokenization and lemmatization, both of which were performed using packages in the NLTK suite.\n","\n","Tokenization converts text to analysis relevant word tokens, while lemmatization transforms words to a simpler form, returning the word’s lemma – a canonical form of all its inflectional forms (for example, go represents its inflected forms of goes, going, went, gone).\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df['Tweet'] = df['Tweet'].apply(word_tokenize)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def lemma_wordnet(input):\n","    lem = WordNetLemmatizer()\n","    return [lem.lemmatize(w) for w in input]\n","\n","df['Tweet'] = df['Tweet'].apply(lemma_wordnet)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Vectorization\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Here we explore two techniques for encoding text data into numerical vectors. This is to extract suitable features from processed texts. Both will be implemented using scikitlearn’s CountVectorizer, where `ngram_range` will be set to (1,1) for BoW and (2,2) for B-BoW.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Bag-of-Words (BoW) is a vectorization method to convert variable-length texts into fixed-length vectors, without considering the semantic relation between words.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["BoW often results in a very sparse representation. For a large dataset with the vocabulary of a few thousand words, preprocessing text before employing BoW can be useful.\n"]},{"cell_type":"code","execution_count":99,"metadata":{},"outputs":[],"source":["# combine individual words\n","\n","def combine_text(input):\n","    combined = ' '.join(input)\n","    return combined\n","\n","df['Tweet'] = df['Tweet'].apply(combine_text)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We can plot some of the values we see they are zero\n"]},{"cell_type":"code","execution_count":100,"metadata":{},"outputs":[{"ename":"ValueError","evalue":"empty vocabulary; perhaps the documents only contain stop words","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[100], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m cv \u001b[39m=\u001b[39m CountVectorizer(ngram_range\u001b[39m=\u001b[39m(\u001b[39m1\u001b[39m, \u001b[39m1\u001b[39m))\n\u001b[0;32m----> 2\u001b[0m X_train_bow \u001b[39m=\u001b[39m cv\u001b[39m.\u001b[39;49mfit_transform(df[\u001b[39m'\u001b[39;49m\u001b[39mTweet\u001b[39;49m\u001b[39m'\u001b[39;49m])\n\u001b[1;32m      3\u001b[0m \u001b[39mprint\u001b[39m(X_train_bow[\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m,\u001b[39m0\u001b[39m:\u001b[39m10\u001b[39m]\u001b[39m.\u001b[39mtodense())\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1338\u001b[0m, in \u001b[0;36mCountVectorizer.fit_transform\u001b[0;34m(self, raw_documents, y)\u001b[0m\n\u001b[1;32m   1330\u001b[0m             warnings\u001b[39m.\u001b[39mwarn(\n\u001b[1;32m   1331\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39mUpper case characters found in\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1332\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m vocabulary while \u001b[39m\u001b[39m'\u001b[39m\u001b[39mlowercase\u001b[39m\u001b[39m'\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1333\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m is True. These entries will not\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1334\u001b[0m                 \u001b[39m\"\u001b[39m\u001b[39m be matched with any documents\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1335\u001b[0m             )\n\u001b[1;32m   1336\u001b[0m             \u001b[39mbreak\u001b[39;00m\n\u001b[0;32m-> 1338\u001b[0m vocabulary, X \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_count_vocab(raw_documents, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfixed_vocabulary_)\n\u001b[1;32m   1340\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mbinary:\n\u001b[1;32m   1341\u001b[0m     X\u001b[39m.\u001b[39mdata\u001b[39m.\u001b[39mfill(\u001b[39m1\u001b[39m)\n","File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.10/lib/python3.10/site-packages/sklearn/feature_extraction/text.py:1228\u001b[0m, in \u001b[0;36mCountVectorizer._count_vocab\u001b[0;34m(self, raw_documents, fixed_vocab)\u001b[0m\n\u001b[1;32m   1226\u001b[0m     vocabulary \u001b[39m=\u001b[39m \u001b[39mdict\u001b[39m(vocabulary)\n\u001b[1;32m   1227\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m vocabulary:\n\u001b[0;32m-> 1228\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1229\u001b[0m             \u001b[39m\"\u001b[39m\u001b[39mempty vocabulary; perhaps the documents only contain stop words\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1230\u001b[0m         )\n\u001b[1;32m   1232\u001b[0m \u001b[39mif\u001b[39;00m indptr[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m] \u001b[39m>\u001b[39m np\u001b[39m.\u001b[39miinfo(np\u001b[39m.\u001b[39mint32)\u001b[39m.\u001b[39mmax:  \u001b[39m# = 2**31 - 1\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m     \u001b[39mif\u001b[39;00m _IS_32BIT:\n","\u001b[0;31mValueError\u001b[0m: empty vocabulary; perhaps the documents only contain stop words"]}],"source":["cv = CountVectorizer(ngram_range=(1, 1))\n","X_train_bow = cv.fit_transform(df['Tweet'])\n","print(X_train_bow[0:10,0:10].todense())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv = CountVectorizer(ngram_range=(1, 1))\n","X_train_bow = cv.fit_transform(df['Tweet'].values.tolist()) \n","Y_train_bow = df['Sentiment']\n","Y_train_bow"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Bigram BoW (B-BoW) represents a text document as a weakly ordered collection of contiguous sequences but of two items. It allows for the preservation of more word locality information.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["cv_bbow = CountVectorizer(ngram_range=(2, 2))\n","X_train_bbow = cv_bbow.fit_transform(df['Tweet']) \n","Y_train_bbow = df['Sentiment']\n","Y_train_bbow"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Term frequency-inverse document frequency (TF-IDF) is a frequency-based embedding method that\n","measures the importance of a word in a given document. It can be implemented using the `TfidfVectorizer` function from sklearn.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["vectorizer = TfidfVectorizer(use_idf = True, ngram_range=(1, 1))\n","vectorizer.fit(df)\n","X_train_tfidf = vectorizer.fit_transform(df['Tweet'])\n","Y_train_tfidf = df['Sentiment'] \n","Y_train_tfidf "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Example 1 - Working with Speech Commands\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this example, we will walk through how to pre-process audio files in the .WAV format. We will be using a portion of the [Speech Commands dataset (Warden, 2018)](https://www.tensorflow.org/datasets/catalog/speech_commands?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera747-2022-01-01), which contains short (one-second or less) audio clips of commands, such as \"down\", \"go\", \"left\", \"no\", \"right\", \"stop\", \"up\", and \"yes\".\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise A - Import data set\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us start by importing a portion of the Speech Commands dataset. The original dataset consists of over 105,000 audio files in the WAV (Waveform) audio file format of people saying 35 different words.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["A portion of the dataset is provided by [Tensorflow](http://storage.googleapis.com/download.tensorflow.org/data/mini_speech_commands.zip?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) as a .zip file. We have modified it further to include just a few test files.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/datasets/mini_speech_commands.zip\")\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If you open up the data path, you will see that the dataset's audio clips are stored in eight folders corresponding to each speech command: no, yes, down, go, left, up, right, and stop.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise B - Extract audio clips\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Start off by listing the commands by reading the names of the eight folders that were created earlier.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here:\n","data_dir=\"mini_speech_commands\"\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","commands = np.array(tf.io.gfile.listdir(str(data_dir)))\n","commands = commands[commands != './data/mini_speech_commands/README.md']\n","commands\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we will extract the names/paths of the audio clips into a file:\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["filenames = tf.io.gfile.glob(str(data_dir) + '/*/*')\n","filenames"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Print out the number of audio clips present.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","num_samples = len(filenames)\n","print(num_samples)\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise C - Plotting audio files\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let's plot an audio waveforms by converting it to a numpy array. We will first pick a test file.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from scipy.io.wavfile import read\n","test_file_name = '/no/97f4c236_nohash_3.wav'"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now use the `read` function from `scipy.io.wavfile` to read in the wav file as an array.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","rate, test_file = read(data_dir+test_file_name)\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Plot the test file.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n","plt.plot(test_file)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","plt.plot(test_file)\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise D - Basic pre-processing\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We will learn about audio clip pre-processing. Each .WAV file contains time-series data with a set number of samples per second. Each sample represents the amplitude of the audio signal at that specific time. In this dataset, the sampling rate of each clip is 16kHz.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We will use the `tf.audio.decode_wav` function to decode the 16-bit WAV files to float tensors, so they are in an easy to use format. The -32768 to 32767 signed 16-bit values will be scaled to -1.0 to 1.0 in float. Let's start off by printing the shape of the tensor returned by tf.audio.decode_wav. It should be \\[samples, channels], where channels is 1 for mono or 2 for stereo. The Speech Commands dataset only contains mono recordings.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We will first start by reading the test file as a tensor object.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["test_file_tensor = tf.io.read_file(data_dir+'/no/97f4c236_nohash_3.wav')\n","test_audio, _ = tf.audio.decode_wav(contents=test_file_tensor)\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","test_audio, _ = tf.audio.decode_wav(contents=test_file_tensor)\n","print(test_audio.shape)\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise E - Spectrogram\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Waveforms in this dataset are represented in the time domain.\n","\n","Let us look into transforming the waveforms from the time-domain signals into the time-frequency-domain signals by converting it to a spectrogram. A spectrogram shows frequency changes over time and can be represented as 2D images. These images can even be used as input to a neural network.\n","\n","We will use the `spectrogram` function from `scipy` to do this.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["frequencies, times, spectrogram = signal.spectrogram(test_file, rate)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use the `pcolormesh` and `imshow` functions with time, frequencies, and spectrogram as input to plot the spectrogram.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","plt.pcolormesh(times, frequencies, spectrogram)\n","plt.imshow(spectrogram)\n","plt.ylabel('Frequency [Hz]')\n","plt.xlabel('Time [sec]')\n","plt.show()\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["# Example 2 - Gene Family Classification\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this example, we will train a Naïve Bayes (NB) classifier on a human DNA sequence dataset to predict the gene family class label. The dataset contains seven different classes shown in the table below.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["| gene family                   | number | class label |\n","| ----------------------------- | ------ | ----------- |\n","| *G-protein coupled receptors* | 531    | 0           |\n","| *Tyrosine kynase*             | 534    | 1           |\n","| *Tyrosine phosphatase*        | 349    | 2           |\n","| *Synthetase*                  | 672    | 3           |\n","| *Synthase*                    | 711    | 4           |\n","| *Ion channel*                 | 240    | 5           |\n","| *Transcription factor*        | 1343   | 6           |\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise A: Load human DNA data\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["await skillsnetwork.prepare(\"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-ML311-Coursera/labs/Module4/L1/human_data.txt\", overwrite=True)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Let us start by reading the tab-separated `human_data.txt` file using Pandas.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","human_data = pd.read_csv(\"human_data.txt\", sep=\"\\t\")\n","```\n","\n","</details>\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["human_data.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, we will create a plot to view the class distribution in our data.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["human_data['class'].value_counts().sort_index().plot.bar()\n","plt.title(\"Class distribution of Human DNA\")"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise B: Sequence of Characters to K-mers\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Most ML classification and regression methods expect for the input to be in form of sequences of equal lengths. In this exercise, we will take long biological sequences and break them down into k-mer length overlapping sub-sequences. For example, imagine we have the following sequence:\n","\n","GTGCCCAGGT\n","\n","We will write a function that will convert this sequence into overlapping k-mer words for a specified length, or k.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def kmers_funct(seq, size=6):\n","    return [seq[x:x+size].lower() for x in range(len(seq) - size + 1)]"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["If we feed our sequence of length 10 to this function, we get the following 5 k-mers.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["kmers_funct('GTGCCCAGGT')"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now, use the `apply()` to apply this function to the entire `human_data`.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","human_data['words'] = human_data.apply(lambda x: kmers_funct(x['sequence']), axis=1)\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To clean up the resulting dataset, we will drop the `sequence` column.\n","\n","Let us view the first 5 rows.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["human_data = human_data.drop('sequence', axis=1)\n","human_data.head()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As seen above, the DNA sequence is now changed to lowercase. It has been split into all possible k-mer words of length 6.\n","\n","We will now join the sub-sequences into a “sentence\", as is normally done in many NLP tasks.\n","\n","This will be done by converting the lists of k-mers for each gene into string sentences of words. We will then use these to create a Bag of Words (BoW) model, to extract a feature matrix from the DNA sequences.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["human_texts = list(human_data['words'])\n","for item in range(len(human_texts)):\n","    human_texts[item] = ' '.join(human_texts[item])\n","#separate labels\n","y_human = human_data.iloc[:, 0].values"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The target variable contains an array of class values.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["print(\"length of human text seq.:\", len(human_texts))\n","print(\"length of labels\", y_human.shape)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise C: Bag-of-Words\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In this exercise, you will use the `CountVectorizer()` function to create the Bag of Words model. Use an n-gram count of 4.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.feature_extraction.text import CountVectorizer"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","cv = CountVectorizer(ngram_range=(4,4)) \n","X = cv.fit_transform(human_texts)\n","\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Exercise D: Multinomial Naïve Bayes Classifier\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["In the steps above, we learned how to transform the raw DNA sequences into uniform length numerical vectors in the form of k-mer counts and ngrams.\n","\n","Now, we can go ahead and build a Naïve Bayes classification model for predicting the gene class based on the sequence.\n","\n","We will hold out 20% of the human data to test our model.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Use `train_test_split` from `sklearn` to split the dataset.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.model_selection import train_test_split"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","X_train, X_test, y_train, y_test = train_test_split(X, \n","                                                    y_human, \n","                                                    test_size = 0.20, \n","                                                    random_state=42)\n","\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Next, build a simple multinomial naive Bayes classifier.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.naive_bayes import MultinomialNB"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","classifier = MultinomialNB(alpha=0.1)\n","classifier.fit(X_train, y_train)\n","\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Finally, we will make predictions on the held out test set, and print out a classification report.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from sklearn.metrics import classification_report"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# Write your solution here \n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<details>\n","    <summary>Click here for Solution</summary>\n","\n","```python\n","y_pred = classifier.predict(X_test)\n","print(classification_report(y_test, y_pred))\n","\n","```\n","\n","</details>\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Authors\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["[Kopal Garg](https://www.linkedin.com/in/gargkopal/?utm_medium=Exinfluencer&utm_source=Exinfluencer&utm_content=000026UJ&utm_term=10006555&utm_id=NA-SkillsNetwork-Channel-SkillsNetworkCoursesIBMDeveloperSkillsNetworkML311Coursera35714171-2022-01-01) is a Masters student in Computer Science at the University of Toronto.\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Change Log\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["| Date (YYYY-MM-DD) | Version | Changed By | Change Description    |\n","| ----------------- | ------- | ---------- | --------------------- |\n","| 2022-06-23        | 0.1     | Kopal Garg | Create Lab            |\n","| 2022-08-02        | 0.2     | Kopal Garg | Added another example |\n","| 2022-09-07        | 0.2     | Steve Hord | QA pass edits         |\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Copyright © 2022 IBM Corporation. All rights reserved.\n"]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.11"}},"nbformat":4,"nbformat_minor":4}
